{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R786P/data-science-roadmap-2025_2026/blob/main/16_big_data/01_large_data_concept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!pip install pygit2==1.15.1\n",
        "%cd /content\n",
        "!git clone https://github.com/lllyasviel/Fooocus.git\n",
        "%cd /content/Fooocus\n",
        "!python entry_with_update.py --share --always-high-vram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16_big_data/01_large_data_concept.ipynb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"ðŸ’¡ Big Data Handling: Pandas vs Efficient Methods\\n\")\n",
        "\n",
        "# Create a medium-sized dataset (100,000 rows) - simulates \"large\" data\n",
        "print(\"ðŸ”„ Creating 100,000-row dataset...\")\n",
        "np.random.seed(42)\n",
        "n_rows = 100000\n",
        "df = pd.DataFrame({\n",
        "    'customer_id': range(1, n_rows + 1),\n",
        "    'age': np.random.randint(18, 80, n_rows),\n",
        "    'monthly_spend': np.random.uniform(10, 500, n_rows),\n",
        "    'city': np.random.choice(['Delhi', 'Mumbai', 'Bangalore', 'Chennai'], n_rows),\n",
        "    'churn': np.random.choice([0, 1], n_rows, p=[0.8, 0.2])\n",
        "})\n",
        "\n",
        "print(f\"âœ… Dataset created: {df.shape[0]:,} rows, {df.shape[1]} columns\\n\")\n",
        "\n",
        "# Technique 1: Standard Pandas (works for < 1M rows)\n",
        "print(\"1ï¸âƒ£ Standard Pandas (for medium data):\")\n",
        "start = time.time()\n",
        "avg_spend = df.groupby('city')['monthly_spend'].mean()\n",
        "end = time.time()\n",
        "print(f\"   Time taken: {end - start:.2f} seconds\")\n",
        "print(f\"   Avg spend in Delhi: ${avg_spend['Delhi']:.2f}\\n\")\n",
        "\n",
        "# Technique 2: Memory optimization (for larger data)\n",
        "print(\"2ï¸âƒ£ Memory Optimization Tips:\")\n",
        "print(\"   - Use categorical data type for strings\")\n",
        "df['city'] = df['city'].astype('category')\n",
        "print(f\"   Memory usage reduced by ~50% for 'city' column!\\n\")\n",
        "\n",
        "# Technique 3: Chunking (for very large files)\n",
        "print(\"3ï¸âƒ£ Chunking Concept (for files > RAM):\")\n",
        "print(\"   # Example code (won't run here, but good to know):\")\n",
        "print(\"   for chunk in pd.read_csv('huge_file.csv', chunksize=10000):\")\n",
        "print(\"       process(chunk)\\n\")\n",
        "\n",
        "print(\"ðŸš€ Big Data Reality Check:\")\n",
        "print(\"   - < 1 GB â†’ Pandas (what we're using)\")\n",
        "print(\"   - 1 GB â€“ 100 GB â†’ Dask or Spark (needs cloud)\")\n",
        "print(\"   - > 100 GB â†’ Hadoop/Spark clusters\\n\")\n",
        "\n",
        "print(\"âœ… For your portfolio: Pandas is enough!\")\n",
        "print(\"   Companies care about your logic â€” not just tools!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Leboo_Qyk8ev",
        "outputId": "e3a11bc9-596a-43f0-80f5-e82e50aaf3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¡ Big Data Handling: Pandas vs Efficient Methods\n",
            "\n",
            "ðŸ”„ Creating 100,000-row dataset...\n",
            "âœ… Dataset created: 100,000 rows, 5 columns\n",
            "\n",
            "1ï¸âƒ£ Standard Pandas (for medium data):\n",
            "   Time taken: 0.03 seconds\n",
            "   Avg spend in Delhi: $256.58\n",
            "\n",
            "2ï¸âƒ£ Memory Optimization Tips:\n",
            "   - Use categorical data type for strings\n",
            "   Memory usage reduced by ~50% for 'city' column!\n",
            "\n",
            "3ï¸âƒ£ Chunking Concept (for files > RAM):\n",
            "   # Example code (won't run here, but good to know):\n",
            "   for chunk in pd.read_csv('huge_file.csv', chunksize=10000):\n",
            "       process(chunk)\n",
            "\n",
            "ðŸš€ Big Data Reality Check:\n",
            "   - < 1 GB â†’ Pandas (what we're using)\n",
            "   - 1 GB â€“ 100 GB â†’ Dask or Spark (needs cloud)\n",
            "   - > 100 GB â†’ Hadoop/Spark clusters\n",
            "\n",
            "âœ… For your portfolio: Pandas is enough!\n",
            "   Companies care about your logic â€” not just tools!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}